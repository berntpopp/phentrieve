# Phentrieve-Specific Multilingual Embedding Model Fine-Tuning Plan

## 1. Executive Summary

**Goal:** To create a state-of-the-art, multilingual, domain-specific sentence embedding model tailored for the `phentrieve` project.

**Method:** Fine-tune a powerful, pre-trained multilingual base model using a custom-built training corpus derived from the Human Phenotype Ontology (HPO).

**Key Innovation:** The training corpus will be generated by programmatically translating HPO terms, definitions, and synonyms into multiple target languages. This will enable the model to learn deep semantic relationships between medical concepts both within and across languages.

**Expected Outcome:** A specialized `phentrieve-model` that significantly outperforms general-purpose models on multilingual and cross-lingual phenotype retrieval tasks, leading to higher accuracy for end-users.

## 2. Rationale

Off-the-shelf embedding models present a trade-off:
- **General Multilingual Models** (e.g., `bge-m3`, `mxbai-embed-large-v1`) handle many languages but lack deep, specialized knowledge of the biomedical domain.
- **Biomedical Models** (e.g., `BioLORD`, `SapBERT`) have deep domain knowledge but are almost exclusively trained on English text, limiting their utility for international use cases.

This project will bridge that gap by creating a single model that is an expert in both, directly addressing the core need of the `phentrieve` project.

## 3. Methodology

### 3.1. Data Acquisition and Preparation

1.  **Source:** The official HPO data file (e.g., `hp.obo`).
2.  **Extraction:** Develop a Python script to parse the HPO file. For each term, extract:
    - HPO ID (e.g., `HP:0001369`)
    - Term Name (e.g., "Athetosis")
    - Definition
    - Synonyms
3.  **Translation:**
    - **Target Languages:** Define a set of target languages (e.g., German, French, Spanish, Japanese, Chinese).
    - **API Integration:** Use a robust translation service (e.g., DeepL API, Google Cloud Translation API) to translate the extracted text fields for each term.
    - **Storage:** Store the translations in a structured format, such as a directory of JSON files where each file is named after the HPO ID (e.g., `HP_0001369.json`) and contains both the original English and all translated text fields.

### 3.2. Training Corpus Generation

The goal is to create "positive pairs" of text that represent the same underlying concept. These pairs will be used to train the model via contrastive learning.

**Types of Pairs to Generate:**

-   **Term-Definition (Within-Language):** `("Athetosis", "A type of dyskinesia...")`, `("Athetose", "Eine Art von Dyskinesie...")`
-   **Term-Synonym (Within-Language):** `("Athetosis", "Athetoid movements")`, `("Athetose", "Athetoide Bewegungen")`
-   **Cross-Lingual Term Alignment:** `("Athetosis", "Athetose")`
-   **Cross-Lingual Definition Alignment:** `("A type of dyskinesia...", "Eine Art von Dyskinesie...")`
-   **Cross-Lingual Term-Definition Combination:** `("Athetosis", "Eine Art von Dyskinesie...")`

A script will iterate through the stored JSON data to generate a final training file containing a large list of these pairs, formatted as `sentence_transformers.InputExample` objects.

### 3.3. Model Selection

-   **Base Model:** `BAAI/bge-m3-large`
-   **Rationale:** This model offers top-tier multilingual capabilities (over 100 languages) and has a large capacity, making it an ideal foundation for domain adaptation. Its size and performance are manageable for fine-tuning with the available GPU hardware (RTX 5090 / Cluster).
-   **Alternative:** `mixedbread-ai/mxbai-embed-large-v1` can be used for faster iteration cycles if needed.

### 3.4. Fine-Tuning Process

-   **Framework:** Python with the `sentence-transformers` library.
-   **Hardware:** High-VRAM GPU (RTX 5090 or equivalent).
-   **Loss Function:** `losses.MultipleNegativesRankingLoss`. This is a highly efficient and effective contrastive loss function that does not require explicit negative examples.
-   **Key Hyperparameters:**
    -   `batch_size`: 32 or 64 (to be tuned based on VRAM).
    -   `epochs`: 1-3.
    -   `learning_rate`: `2e-5` (a standard starting point).
    -   `warmup_steps`: 10% of total training steps.

## 4. Evaluation

Success will be measured both intrinsically and extrinsically.

1.  **Intrinsic Evaluation (During Training):**
    -   A hold-out test set of ~1,000 positive pairs will be created.
    -   `evaluation.EmbeddingSimilarityEvaluator` will be used to monitor the model's ability to produce high cosine similarity scores for these pairs during training, helping to prevent overfitting and save the best checkpoint.

2.  **Extrinsic Evaluation (Task-Based):**
    -   The fine-tuned model will be integrated into the existing `phentrieve` benchmarking framework.
    -   **Primary Metrics:** The model's performance will be measured against the standard test suite using Mean Reciprocal Rank (MRR) and Hits@K (HR@1, HR@3, HR@10).
    -   **Comparison:** The results will be directly compared against the original `BAAI/bge-m3-large` and all other pre-trained models currently benchmarked. The primary goal is to demonstrate a statistically significant improvement.

## 5. Deployment and Optimization

The fine-tuned model must be optimized to run efficiently on the target production environment (CPU-based VPS with 8GB RAM).

1.  **Export:** The fine-tuned model will be saved from the training process.
2.  **Optimization:** A post-processing script will be created to:
    -   **Export to ONNX:** Convert the PyTorch model to the ONNX format using `Hugging Face Optimum`.
    -   **Quantize:** Apply dynamic 8-bit integer quantization (INT8) during the ONNX export.
3.  **Result:** This process will create a smaller, faster model file that is highly optimized for CPU inference, making it suitable for deployment on the VPS while retaining most of the accuracy gains from fine-tuning.

---