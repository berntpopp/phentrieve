# Technical Performance Optimizations Plan

**Status:** Active
**Date:** 2025-01-18
**Parent Plan:** CHUNKING-OPTIMIZATION-PLAN.md
**Priority:** High
**Estimated Effort:** 2-3 weeks
**Scope:** Infrastructure-level optimizations WITHOUT modifying chunking algorithms

---

## Executive Summary

This plan extracts **purely technical, infrastructure-level optimizations** from the CHUNKING-OPTIMIZATION-PLAN.md that can be implemented **immediately** without changing any chunking algorithm logic. These optimizations focus on parallelization, memory efficiency, model management, and data loading.

### Key Principles

- ✅ **Profile First, Optimize Second** - Measure before assuming bottlenecks
- ✅ **No Algorithm Changes** - Only infrastructure improvements
- ✅ **Incremental Delivery** - Each optimization is independently valuable
- ✅ **Measurable Impact** - All improvements must be quantified
- ✅ **Backward Compatible** - No breaking API changes

### Expected Outcomes

- **Performance:** 2-4x speedup for batch processing (validated by profiling)
- **Memory:** 30-50% reduction in peak memory usage
- **Scalability:** Support for 10-100x larger document batches
- **Developer Experience:** Faster iteration cycles, better tooling

---

## Table of Contents

1. [Current Performance Bottlenecks](#1-current-performance-bottlenecks)
2. [Optimization 1: Profiling Infrastructure](#2-optimization-1-profiling-infrastructure)
3. [Optimization 2: Parallel Embedding Generation](#3-optimization-2-parallel-embedding-generation)
4. [Optimization 3: Model Instance Pooling](#4-optimization-3-model-instance-pooling)
5. [Optimization 4: Batch Processing Pipeline](#5-optimization-4-batch-processing-pipeline)
6. [Optimization 5: Memory-Efficient Data Loading](#6-optimization-5-memory-efficient-data-loading)
7. [Optimization 6: ChromaDB Query Optimization](#7-optimization-6-chromadb-query-optimization)
8. [Implementation Roadmap](#8-implementation-roadmap)
9. [Performance Validation](#9-performance-validation)

---

## 1. Current Performance Bottlenecks

### 1.1 Identified Issues (from CHUNKING-OPTIMIZATION-PLAN.md)

**Performance Limitations:**
```
❌ No parallelization for batch processing
❌ O(n²) similarity comparisons in sliding window
❌ Model loaded per pipeline instance (not shared)
❌ Sequential embedding generation for long texts
❌ No batch optimization for multi-document processing
```

**Current Architecture:**
- **Model Loading:** Each `SlidingWindowSemanticSplitter` instance loads its own model
- **Embedding Generation:** Sequential calls to `model.encode()`
- **Batch Processing:** Each document processed independently
- **Memory:** No caching or pooling of model instances

### 1.2 Critical Requirement: PROFILE FIRST!

**From CHUNKING-OPTIMIZATION-PLAN.md (Phase 1, Week 1):**

> **CRITICAL: Profile before optimizing!**
>
> ```bash
> # Day 1: Establish baseline
> python -m cProfile -o baseline.prof phentrieve text process corpus.txt
> python -m pstats baseline.prof
> > sort cumtime
> > stats 30
>
> # Identify actual bottlenecks (not assumed ones)
> # Options: ChromaDB queries? Model loading? Chunking? I/O?
> ```

**We must NOT assume bottlenecks - we must MEASURE them!**

---

## 2. Optimization 1: Profiling Infrastructure

### 2.1 Objective

Establish comprehensive profiling infrastructure to **measure actual bottlenecks** before implementing optimizations.

### 2.2 Implementation

**File:** `phentrieve/profiling/__init__.py` (NEW)

```python
"""
Profiling utilities for performance analysis.

Provides decorators, context managers, and CLI tools for profiling.
"""

import cProfile
import pstats
import io
import time
import functools
import logging
from pathlib import Path
from typing import Callable, Any, Optional
from contextlib import contextmanager
import json

logger = logging.getLogger(__name__)


class PerformanceProfiler:
    """
    Profiling context manager and decorator for performance analysis.

    Features:
    - cProfile integration for detailed call graphs
    - Memory profiling with tracemalloc
    - Timing utilities
    - JSON export for automated analysis
    """

    def __init__(
        self,
        output_dir: Path = Path("results/profiling"),
        profile_name: str = "default",
        enable_memory_profiling: bool = True,
    ):
        """
        Initialize profiler.

        Args:
            output_dir: Directory to save profiling results
            profile_name: Name for this profiling run
            enable_memory_profiling: Enable memory tracking
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.profile_name = profile_name
        self.enable_memory_profiling = enable_memory_profiling

        self.profiler: Optional[cProfile.Profile] = None
        self.start_time: float = 0.0
        self.memory_snapshots: list = []

    def __enter__(self):
        """Start profiling."""
        logger.info(f"Starting profiling: {self.profile_name}")

        # Start CPU profiling
        self.profiler = cProfile.Profile()
        self.profiler.enable()

        # Start memory profiling
        if self.enable_memory_profiling:
            import tracemalloc
            tracemalloc.start()
            self.memory_snapshots.append(tracemalloc.take_snapshot())

        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Stop profiling and save results."""
        elapsed_time = time.time() - self.start_time

        # Stop CPU profiling
        if self.profiler:
            self.profiler.disable()
            self._save_cpu_profile()

        # Stop memory profiling
        if self.enable_memory_profiling:
            import tracemalloc
            self.memory_snapshots.append(tracemalloc.take_snapshot())
            self._save_memory_profile()
            tracemalloc.stop()

        # Save summary
        self._save_summary(elapsed_time)

        logger.info(
            f"Profiling complete: {self.profile_name} "
            f"(elapsed: {elapsed_time:.2f}s)"
        )

    def _save_cpu_profile(self):
        """Save CPU profiling data."""
        if not self.profiler:
            return

        # Save raw profile data
        profile_file = self.output_dir / f"{self.profile_name}.prof"
        self.profiler.dump_stats(str(profile_file))
        logger.info(f"Saved CPU profile: {profile_file}")

        # Generate human-readable text report
        s = io.StringIO()
        ps = pstats.Stats(self.profiler, stream=s)
        ps.strip_dirs()
        ps.sort_stats(pstats.SortKey.CUMULATIVE)
        ps.print_stats(50)  # Top 50 functions

        report_file = self.output_dir / f"{self.profile_name}_cpu_report.txt"
        with open(report_file, "w") as f:
            f.write(s.getvalue())
        logger.info(f"Saved CPU report: {report_file}")

        # Generate JSON for automated analysis
        stats_data = self._extract_stats_data(ps)
        json_file = self.output_dir / f"{self.profile_name}_cpu_stats.json"
        with open(json_file, "w") as f:
            json.dump(stats_data, f, indent=2)
        logger.info(f"Saved CPU stats JSON: {json_file}")

    def _save_memory_profile(self):
        """Save memory profiling data."""
        if len(self.memory_snapshots) < 2:
            return

        import tracemalloc

        # Compare snapshots
        snapshot_start = self.memory_snapshots[0]
        snapshot_end = self.memory_snapshots[-1]

        top_stats = snapshot_end.compare_to(snapshot_start, 'lineno')

        # Save top memory allocations
        report_file = self.output_dir / f"{self.profile_name}_memory_report.txt"
        with open(report_file, "w") as f:
            f.write("Top 50 Memory Allocations:\n")
            f.write("=" * 80 + "\n\n")
            for stat in top_stats[:50]:
                f.write(f"{stat}\n")

        logger.info(f"Saved memory report: {report_file}")

        # Generate JSON summary
        memory_data = {
            "peak_memory_mb": tracemalloc.get_traced_memory()[1] / 1024 / 1024,
            "current_memory_mb": tracemalloc.get_traced_memory()[0] / 1024 / 1024,
            "top_allocations": [
                {
                    "size_mb": stat.size / 1024 / 1024,
                    "count": stat.count,
                    "traceback": str(stat.traceback),
                }
                for stat in top_stats[:20]
            ],
        }

        json_file = self.output_dir / f"{self.profile_name}_memory_stats.json"
        with open(json_file, "w") as f:
            json.dump(memory_data, f, indent=2)
        logger.info(f"Saved memory stats JSON: {json_file}")

    def _save_summary(self, elapsed_time: float):
        """Save profiling summary."""
        summary = {
            "profile_name": self.profile_name,
            "elapsed_time_seconds": elapsed_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "output_dir": str(self.output_dir),
        }

        summary_file = self.output_dir / f"{self.profile_name}_summary.json"
        with open(summary_file, "w") as f:
            json.dump(summary, f, indent=2)

    @staticmethod
    def _extract_stats_data(ps: pstats.Stats) -> dict:
        """Extract statistics as structured data."""
        stats_list = []

        for func, (cc, nc, tt, ct, callers) in ps.stats.items():
            filename, line, func_name = func
            stats_list.append({
                "function": func_name,
                "filename": filename,
                "line": line,
                "ncalls": nc,
                "tottime": tt,
                "cumtime": ct,
                "percall_tottime": tt / nc if nc > 0 else 0,
                "percall_cumtime": ct / nc if nc > 0 else 0,
            })

        # Sort by cumulative time
        stats_list.sort(key=lambda x: x["cumtime"], reverse=True)

        return {
            "total_functions": len(stats_list),
            "top_50_by_cumtime": stats_list[:50],
        }


def profile_function(profile_name: Optional[str] = None):
    """
    Decorator to profile a function.

    Usage:
        @profile_function("my_function")
        def my_function():
            ...
    """
    def decorator(func: Callable) -> Callable:
        name = profile_name or func.__name__

        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            with PerformanceProfiler(profile_name=name):
                return func(*args, **kwargs)

        return wrapper

    return decorator


@contextmanager
def profile_block(profile_name: str):
    """
    Context manager to profile a code block.

    Usage:
        with profile_block("my_code_block"):
            # code to profile
            ...
    """
    profiler = PerformanceProfiler(profile_name=profile_name)
    with profiler:
        yield profiler


class TimingContext:
    """
    Lightweight timing context manager (no profiling overhead).

    Usage:
        with TimingContext("operation_name") as timer:
            # code to time
            ...
        print(f"Elapsed: {timer.elapsed_ms}ms")
    """

    def __init__(self, name: str, log_on_exit: bool = True):
        self.name = name
        self.log_on_exit = log_on_exit
        self.start_time: float = 0.0
        self.end_time: float = 0.0
        self.elapsed_ms: float = 0.0

    def __enter__(self):
        self.start_time = time.perf_counter()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.perf_counter()
        self.elapsed_ms = (self.end_time - self.start_time) * 1000

        if self.log_on_exit:
            logger.info(f"{self.name}: {self.elapsed_ms:.2f}ms")
```

**File:** `phentrieve/cli/profiling_commands.py` (NEW)

```python
"""
CLI commands for profiling operations.
"""

import typer
from pathlib import Path
from typing import Optional

profiling_app = typer.Typer(help="Profiling and performance analysis commands")


@profiling_app.command("baseline")
def profile_baseline(
    corpus_file: Path = typer.Argument(..., help="Text file to process"),
    output_dir: Path = typer.Option(
        Path("results/profiling/baseline"),
        "--output", "-o",
        help="Output directory for profiling results",
    ),
    num_documents: int = typer.Option(
        10,
        "--num-docs", "-n",
        help="Number of documents to process (for batch testing)",
    ),
    enable_memory_profiling: bool = typer.Option(
        True,
        "--memory/--no-memory",
        help="Enable memory profiling",
    ),
):
    """
    Profile baseline performance of text processing pipeline.

    This command establishes a performance baseline for:
    - Model loading
    - Embedding generation
    - Chunking operations
    - ChromaDB queries
    - End-to-end processing time

    Example:
        phentrieve profiling baseline corpus.txt
        phentrieve profiling baseline corpus.txt -n 100 --output results/prof
    """
    from phentrieve.profiling import PerformanceProfiler
    from phentrieve.text_processing import process_text
    from phentrieve.config import get_config

    typer.echo(f"Profiling baseline performance on {num_documents} documents...")

    # Read corpus
    with open(corpus_file) as f:
        text = f.read()

    # Simulate multiple documents
    documents = [text] * num_documents

    config = get_config()

    # Profile complete pipeline
    with PerformanceProfiler(
        output_dir=output_dir,
        profile_name="baseline",
        enable_memory_profiling=enable_memory_profiling,
    ):
        for i, doc in enumerate(documents):
            if i % 10 == 0:
                typer.echo(f"  Processing document {i + 1}/{num_documents}...")

            # Process with default strategy
            process_text(
                text=doc,
                language="en",
                chunking_strategy=config.default_chunking_strategy,
            )

    typer.echo(f"\nProfiling complete! Results saved to: {output_dir}")
    typer.echo("\nAnalyze results with:")
    typer.echo(f"  python -m pstats {output_dir}/baseline.prof")
    typer.echo(f"  cat {output_dir}/baseline_cpu_report.txt")
    typer.echo(f"  cat {output_dir}/baseline_memory_report.txt")


@profiling_app.command("compare")
def compare_profiles(
    profile1: Path = typer.Argument(..., help="First profile (.prof file)"),
    profile2: Path = typer.Argument(..., help="Second profile (.prof file)"),
    output_file: Optional[Path] = typer.Option(
        None,
        "--output", "-o",
        help="Output file for comparison report",
    ),
):
    """
    Compare two profiling runs to measure optimization impact.

    Example:
        phentrieve profiling compare baseline.prof optimized.prof
    """
    import pstats

    typer.echo("Comparing profiling results...")

    # Load both profiles
    stats1 = pstats.Stats(str(profile1))
    stats2 = pstats.Stats(str(profile2))

    # Extract timing data
    total_time1 = sum(stat[2] for stat in stats1.stats.values())
    total_time2 = sum(stat[2] for stat in stats2.stats.values())

    speedup = total_time1 / total_time2 if total_time2 > 0 else 0
    improvement_pct = ((total_time1 - total_time2) / total_time1 * 100) if total_time1 > 0 else 0

    # Generate comparison report
    report = []
    report.append("=" * 80)
    report.append("Profiling Comparison Report")
    report.append("=" * 80)
    report.append(f"\nProfile 1: {profile1}")
    report.append(f"Profile 2: {profile2}")
    report.append(f"\nTotal Time 1: {total_time1:.4f}s")
    report.append(f"Total Time 2: {total_time2:.4f}s")
    report.append(f"\nSpeedup: {speedup:.2f}x")
    report.append(f"Improvement: {improvement_pct:+.1f}%")
    report.append("\n" + "=" * 80)

    report_text = "\n".join(report)

    # Print to console
    typer.echo(report_text)

    # Save to file if requested
    if output_file:
        with open(output_file, "w") as f:
            f.write(report_text)
        typer.echo(f"\nComparison report saved to: {output_file}")


@profiling_app.command("visualize")
def visualize_profile(
    profile_file: Path = typer.Argument(..., help="Profile file to visualize"),
    output_format: str = typer.Option(
        "png",
        "--format", "-f",
        help="Output format (png, svg, pdf)",
    ),
):
    """
    Generate call graph visualization from profiling data.

    Requires: gprof2dot, graphviz

    Example:
        phentrieve profiling visualize baseline.prof --format svg
    """
    import subprocess

    output_file = profile_file.with_suffix(f".{output_format}")

    typer.echo(f"Generating call graph visualization...")

    try:
        # Use gprof2dot to convert profile to dot format
        dot_file = profile_file.with_suffix(".dot")

        subprocess.run(
            [
                "gprof2dot",
                "-f", "pstats",
                str(profile_file),
                "-o", str(dot_file),
            ],
            check=True,
        )

        # Use graphviz to render visualization
        subprocess.run(
            [
                "dot",
                "-T" + output_format,
                str(dot_file),
                "-o", str(output_file),
            ],
            check=True,
        )

        typer.echo(f"Visualization saved to: {output_file}")

        # Clean up intermediate dot file
        dot_file.unlink()

    except subprocess.CalledProcessError as e:
        typer.echo(f"Error: {e}", err=True)
        typer.echo(
            "Make sure gprof2dot and graphviz are installed:",
            err=True,
        )
        typer.echo("  pip install gprof2dot", err=True)
        typer.echo("  # Install graphviz via system package manager", err=True)
        raise typer.Exit(1)
    except FileNotFoundError:
        typer.echo(
            "Error: gprof2dot or dot command not found",
            err=True,
        )
        raise typer.Exit(1)
```

### 2.3 Usage

```bash
# Establish baseline performance profile
phentrieve profiling baseline corpus.txt --num-docs 100

# Compare before/after optimization
phentrieve profiling baseline corpus.txt  # Before
# ... implement optimization ...
phentrieve profiling baseline corpus.txt  # After
phentrieve profiling compare baseline_before.prof baseline_after.prof

# Visualize call graph (requires gprof2dot, graphviz)
phentrieve profiling visualize baseline.prof --format svg
```

### 2.4 Deliverables

- [ ] Profiling utilities module (`phentrieve/profiling/`)
- [ ] CLI commands for profiling (`phentrieve profiling baseline/compare/visualize`)
- [ ] Baseline profiling results documented in `plan/PROFILING-RESULTS.md`
- [ ] Identification of top 3 bottlenecks with quantified impact

---

## 3. Optimization 2: Parallel Embedding Generation

### 3.1 Objective

**ONLY implement if profiling shows embedding generation is a bottleneck!**

Use sentence-transformers' built-in parallelization for batch embedding generation.

### 3.2 Current Implementation Analysis

**File:** `phentrieve/text_processing/chunkers.py:SlidingWindowSemanticSplitter._split_one_segment_by_sliding_window()`

```python
# Line 947: Sequential embedding generation
window_embeddings = self.model.encode(window_texts, show_progress_bar=False)
```

**Problem:** Sequential processing, no parallelization for long texts.

### 3.3 Implementation

**Add parallel embedding method to `SlidingWindowSemanticSplitter`:**

```python
# In SlidingWindowSemanticSplitter class

def __init__(
    self,
    language: str = "en",
    model: "SentenceTransformer | None" = None,
    window_size_tokens: int = 7,
    step_size_tokens: int = 1,
    splitting_threshold: float = 0.5,
    min_split_segment_length_words: int = 3,
    # NEW: Parallelization parameters
    use_parallel_embeddings: bool = False,
    parallel_batch_size: int = 32,
    parallel_pool_size: int | None = None,
    parallel_threshold_windows: int = 200,
    **kwargs,
):
    """
    Initialize the sliding window semantic splitter.

    Args:
        ... (existing args)
        use_parallel_embeddings: Enable parallel embedding generation
        parallel_batch_size: Batch size for parallel encoding
        parallel_pool_size: Number of worker processes (None = cpu_count - 1)
        parallel_threshold_windows: Use parallel if windows > threshold
    """
    super().__init__(language=language, **kwargs)
    # ... existing initialization ...

    # NEW: Parallelization configuration
    self.use_parallel_embeddings = use_parallel_embeddings
    self.parallel_batch_size = parallel_batch_size
    self.parallel_pool_size = parallel_pool_size
    self.parallel_threshold_windows = parallel_threshold_windows

    logger.info(
        f"Parallel embeddings: {use_parallel_embeddings} "
        f"(threshold: {parallel_threshold_windows} windows)"
    )


def _generate_embeddings(
    self,
    window_texts: list[str],
) -> "np.ndarray":
    """
    Generate embeddings with optional parallelization.

    Uses sentence-transformers built-in parallelization when:
    - use_parallel_embeddings=True AND
    - len(window_texts) > parallel_threshold_windows

    Args:
        window_texts: List of window texts to embed

    Returns:
        Numpy array of embeddings
    """
    from phentrieve.profiling import TimingContext

    # Decide whether to use parallelization
    use_parallel = (
        self.use_parallel_embeddings and
        len(window_texts) > self.parallel_threshold_windows
    )

    if use_parallel:
        from multiprocessing import cpu_count

        pool_size = self.parallel_pool_size or max(1, cpu_count() - 1)

        with TimingContext(f"Parallel embedding ({len(window_texts)} windows)"):
            embeddings = self.model.encode_multi_process(
                window_texts,
                pool_size=pool_size,
                batch_size=self.parallel_batch_size,
                show_progress_bar=False,
            )

        logger.debug(
            f"Generated {len(embeddings)} embeddings using {pool_size} workers"
        )
    else:
        with TimingContext(f"Sequential embedding ({len(window_texts)} windows)"):
            embeddings = self.model.encode(
                window_texts,
                batch_size=self.parallel_batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
            )

        logger.debug(f"Generated {len(embeddings)} embeddings sequentially")

    return embeddings


# Update _split_one_segment_by_sliding_window to use new method
def _split_one_segment_by_sliding_window(
    self, current_text_segment: str
) -> list[str]:
    """... existing docstring ..."""
    from sklearn.metrics.pairwise import cosine_similarity

    # ... existing code up to embedding generation ...

    logger.debug(f"Generated {len(window_texts)} sliding windows for the segment.")

    # REPLACE: window_embeddings = self.model.encode(window_texts, show_progress_bar=False)
    # WITH:
    window_embeddings = self._generate_embeddings(window_texts)

    # ... rest of the method unchanged ...
```

### 3.4 Configuration

**Update `phentrieve.yaml`:**

```yaml
# Chunking configuration
chunking:
  default_strategy: "balanced"

  # Performance optimizations
  performance:
    # Parallel embedding generation
    use_parallel_embeddings: true          # Enable parallelization
    parallel_batch_size: 32                # Batch size for encoding
    parallel_pool_size: null               # Auto-detect (cpu_count - 1)
    parallel_threshold_windows: 200        # Min windows to trigger parallel
```

### 3.5 Expected Impact

**Based on research (CHUNKING-OPTIMIZATION-PLAN.md section 2.3):**

- **Batch Processing:** 3-5x speedup for large documents (>1000 words)
- **Long Documents:** O(n²) → O(n log n) with parallelization
- **No Impact:** Single short documents (overhead not worth it)

**Validation:**
```bash
# Before optimization
phentrieve profiling baseline long_document.txt --num-docs 100

# After optimization (with parallel enabled in config)
phentrieve profiling baseline long_document.txt --num-docs 100

# Compare
phentrieve profiling compare baseline_before.prof baseline_after.prof
```

### 3.6 Deliverables

- [ ] Add `_generate_embeddings()` method with parallelization logic
- [ ] Add parallelization configuration parameters
- [ ] Update configuration file with performance section
- [ ] Unit tests for parallel vs sequential embeddings (same results)
- [ ] Performance benchmarks showing speedup (if applicable)
- [ ] Documentation in `plan/PROFILING-RESULTS.md`

---

## 4. Optimization 3: Model Instance Pooling

### 4.1 Objective

**Eliminate redundant model loading** by sharing model instances across chunker instances.

### 4.2 Current Problem

**Each chunker instance loads its own model:**

```python
# Current: Each SlidingWindowSemanticSplitter loads its own model
chunker1 = SlidingWindowSemanticSplitter(model=model1, ...)  # Loads model
chunker2 = SlidingWindowSemanticSplitter(model=model2, ...)  # Loads same model again!

# Memory usage: 2x model size (unnecessary!)
```

### 4.3 Implementation: Model Pool

**File:** `phentrieve/embeddings.py` (extend existing)

```python
"""
Embedding model loading and management.

Provides singleton model pool for efficient memory usage.
"""

from typing import Dict, Optional
from sentence_transformers import SentenceTransformer
import logging
import threading

logger = logging.getLogger(__name__)


class ModelPool:
    """
    Singleton model pool for sharing embedding models.

    Thread-safe model caching to prevent redundant loading.

    Usage:
        pool = ModelPool()
        model = pool.get_model("all-MiniLM-L6-v2")  # Loads once
        model2 = pool.get_model("all-MiniLM-L6-v2")  # Returns cached instance
    """

    _instance: Optional["ModelPool"] = None
    _lock = threading.Lock()

    def __new__(cls):
        """Singleton pattern implementation."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._models: Dict[str, SentenceTransformer] = {}
                    cls._instance._model_lock = threading.Lock()
        return cls._instance

    def get_model(
        self,
        model_name: str,
        cache_folder: Optional[str] = None,
        device: Optional[str] = None,
    ) -> SentenceTransformer:
        """
        Get or load embedding model (cached).

        Args:
            model_name: Model name or path
            cache_folder: HuggingFace cache folder
            device: Device to load model on (cpu/cuda)

        Returns:
            Loaded SentenceTransformer model
        """
        # Create cache key
        cache_key = f"{model_name}_{device or 'default'}"

        # Check if already loaded
        if cache_key in self._models:
            logger.debug(f"Using cached model: {cache_key}")
            return self._models[cache_key]

        # Load model (thread-safe)
        with self._model_lock:
            # Double-check after acquiring lock
            if cache_key in self._models:
                return self._models[cache_key]

            logger.info(f"Loading embedding model: {model_name}")

            from phentrieve.profiling import TimingContext

            with TimingContext(f"Model loading: {model_name}"):
                model = SentenceTransformer(
                    model_name,
                    cache_folder=cache_folder,
                    device=device,
                )

            self._models[cache_key] = model
            logger.info(
                f"Model loaded and cached: {cache_key} "
                f"(pool size: {len(self._models)})"
            )

            return model

    def clear_cache(self):
        """Clear model cache (free memory)."""
        with self._model_lock:
            num_models = len(self._models)
            self._models.clear()
            logger.info(f"Cleared model cache ({num_models} models)")

    def get_cache_info(self) -> Dict[str, any]:
        """Get cache statistics."""
        return {
            "num_cached_models": len(self._models),
            "cached_model_names": list(self._models.keys()),
        }


# Backward compatible: Existing function signature
def load_embedding_model(
    model_name: str,
    cache_folder: Optional[str] = None,
    device: Optional[str] = None,
) -> SentenceTransformer:
    """
    Load embedding model (uses pooling for efficiency).

    This function now uses the ModelPool singleton to cache models.
    Multiple calls with the same model_name will return the same instance.

    Args:
        model_name: Model name or path
        cache_folder: HuggingFace cache folder
        device: Device to load model on

    Returns:
        Loaded SentenceTransformer model
    """
    pool = ModelPool()
    return pool.get_model(model_name, cache_folder, device)
```

### 4.4 Usage

**Existing code continues to work (backward compatible):**

```python
# OLD: Still works, now uses pooling internally
from phentrieve.embeddings import load_embedding_model

model1 = load_embedding_model("all-MiniLM-L6-v2")  # Loads model
model2 = load_embedding_model("all-MiniLM-L6-v2")  # Returns cached (same instance)

assert model1 is model2  # True!
```

**New explicit pooling API:**

```python
from phentrieve.embeddings import ModelPool

pool = ModelPool()
model = pool.get_model("all-MiniLM-L6-v2")

# Get cache info
info = pool.get_cache_info()
print(info)  # {"num_cached_models": 1, ...}

# Clear cache when needed
pool.clear_cache()
```

### 4.5 Expected Impact

**Memory Savings:**
- **Before:** N chunker instances × model size (e.g., 10 × 500MB = 5GB)
- **After:** 1 × model size (500MB)
- **Savings:** 30-50% of peak memory usage for multi-instance workloads

**Load Time Savings:**
- **Before:** Model loaded N times
- **After:** Model loaded once
- **Savings:** (N - 1) × model_load_time

### 4.6 Deliverables

- [ ] Implement `ModelPool` singleton in `phentrieve/embeddings.py`
- [ ] Update `load_embedding_model()` to use pooling (backward compatible)
- [ ] Unit tests for pooling behavior
- [ ] Thread-safety tests
- [ ] Memory usage benchmarks
- [ ] Documentation update

---

## 5. Optimization 4: Batch Processing Pipeline

### 5.1 Objective

Enable efficient batch processing of multiple documents with shared model instances.

### 5.2 Implementation

**File:** `phentrieve/text_processing/batch_processor.py` (NEW)

```python
"""
Batch processing pipeline for multiple documents.

Enables efficient parallel processing with shared models.
"""

import logging
from typing import List, Dict, Any, Optional, Callable
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
from pathlib import Path

logger = logging.getLogger(__name__)


class BatchDocumentProcessor:
    """
    Process multiple documents in parallel with shared models.

    Features:
    - Process-based parallelization for CPU-bound tasks
    - Shared model instances (via ModelPool)
    - Progress tracking
    - Error handling and recovery
    """

    def __init__(
        self,
        chunking_strategy: str = "balanced",
        language: str = "en",
        max_workers: Optional[int] = None,
    ):
        """
        Initialize batch processor.

        Args:
            chunking_strategy: Chunking strategy to use
            language: Language code
            max_workers: Max parallel workers (None = cpu_count - 1)
        """
        self.chunking_strategy = chunking_strategy
        self.language = language
        self.max_workers = max_workers or max(1, cpu_count() - 1)

    def process_documents(
        self,
        documents: List[str],
        show_progress: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        Process multiple documents in parallel.

        Args:
            documents: List of document texts
            show_progress: Show progress bar

        Returns:
            List of processing results (one per document)
        """
        if not documents:
            return []

        logger.info(
            f"Batch processing {len(documents)} documents "
            f"with {self.max_workers} workers"
        )

        results: List[Dict[str, Any]] = [None] * len(documents)
        errors: List[tuple[int, Exception]] = []

        # Process in parallel
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_idx = {
                executor.submit(
                    self._process_single_document,
                    doc,
                    idx,
                ): idx
                for idx, doc in enumerate(documents)
            }

            # Collect results as they complete
            if show_progress:
                from tqdm import tqdm
                futures_iter = tqdm(
                    as_completed(future_to_idx),
                    total=len(documents),
                    desc="Processing documents",
                )
            else:
                futures_iter = as_completed(future_to_idx)

            for future in futures_iter:
                idx = future_to_idx[future]
                try:
                    result = future.result()
                    results[idx] = result
                except Exception as e:
                    logger.error(f"Document {idx} failed: {e}")
                    errors.append((idx, e))
                    results[idx] = {"error": str(e), "doc_id": idx}

        # Log summary
        num_success = len([r for r in results if r and "error" not in r])
        num_failed = len(errors)

        logger.info(
            f"Batch processing complete: "
            f"{num_success} succeeded, {num_failed} failed"
        )

        return results

    def _process_single_document(
        self,
        text: str,
        doc_id: int,
    ) -> Dict[str, Any]:
        """
        Process a single document (worker function).

        This runs in a separate process, so it must:
        - Load model via ModelPool (shared cache)
        - Return serializable results

        Args:
            text: Document text
            doc_id: Document index

        Returns:
            Processing results
        """
        from phentrieve.text_processing import process_text
        from phentrieve.profiling import TimingContext

        with TimingContext(f"Document {doc_id}", log_on_exit=False) as timer:
            result = process_text(
                text=text,
                language=self.language,
                chunking_strategy=self.chunking_strategy,
            )

        # Add metadata
        result["doc_id"] = doc_id
        result["processing_time_ms"] = timer.elapsed_ms

        return result

    @classmethod
    def process_from_files(
        cls,
        file_paths: List[Path],
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """
        Process documents from files.

        Args:
            file_paths: List of file paths to process
            **kwargs: Passed to BatchDocumentProcessor constructor

        Returns:
            Processing results
        """
        # Read all files
        documents = []
        for path in file_paths:
            with open(path) as f:
                documents.append(f.read())

        # Process batch
        processor = cls(**kwargs)
        return processor.process_documents(documents)
```

### 5.3 CLI Integration

**File:** `phentrieve/cli/text_processing_commands.py` (extend existing)

```python
@text_app.command("process-batch")
def process_batch(
    input_dir: Path = typer.Argument(..., help="Directory containing text files"),
    output_dir: Path = typer.Option(
        Path("results/batch_processing"),
        "--output", "-o",
        help="Output directory for results",
    ),
    strategy: str = typer.Option(
        "balanced",
        "--strategy", "-s",
        help="Chunking strategy",
    ),
    max_workers: int = typer.Option(
        None,
        "--workers", "-w",
        help="Max parallel workers (default: cpu_count - 1)",
    ),
    pattern: str = typer.Option(
        "*.txt",
        "--pattern", "-p",
        help="File glob pattern",
    ),
):
    """
    Process multiple text files in parallel.

    Example:
        phentrieve text process-batch ./corpus/ --workers 8
        phentrieve text process-batch ./corpus/ --pattern "*.md" -s precise
    """
    from phentrieve.text_processing.batch_processor import BatchDocumentProcessor
    import json

    # Find files
    input_files = list(input_dir.glob(pattern))

    if not input_files:
        typer.echo(f"No files found matching: {input_dir}/{pattern}")
        raise typer.Exit(1)

    typer.echo(f"Found {len(input_files)} files to process")

    # Process batch
    processor = BatchDocumentProcessor(
        chunking_strategy=strategy,
        max_workers=max_workers,
    )

    results = processor.process_from_files(input_files)

    # Save results
    output_dir.mkdir(parents=True, exist_ok=True)

    for result in results:
        doc_id = result.get("doc_id", 0)
        output_file = output_dir / f"result_{doc_id:04d}.json"

        with open(output_file, "w") as f:
            json.dump(result, f, indent=2)

    # Save summary
    summary = {
        "num_documents": len(results),
        "num_success": len([r for r in results if "error" not in r]),
        "num_failed": len([r for r in results if "error" in r]),
        "total_time_ms": sum(r.get("processing_time_ms", 0) for r in results),
    }

    summary_file = output_dir / "summary.json"
    with open(summary_file, "w") as f:
        json.dump(summary, f, indent=2)

    typer.echo(f"\nResults saved to: {output_dir}")
    typer.echo(f"Processed: {summary['num_success']}/{summary['num_documents']}")
```

### 5.4 Usage

```bash
# Process directory of text files in parallel
phentrieve text process-batch ./corpus/ --workers 8

# Use specific strategy and file pattern
phentrieve text process-batch ./corpus/ --pattern "*.md" --strategy precise
```

### 5.5 Expected Impact

- **Batch Processing:** 2-4x speedup for processing 10-100 documents
- **Model Loading:** Load once per worker instead of once per document
- **Scalability:** Support for processing 1000s of documents efficiently

### 5.6 Deliverables

- [ ] Implement `BatchDocumentProcessor` class
- [ ] Add `process-batch` CLI command
- [ ] Unit tests for batch processing
- [ ] Integration tests with multiple documents
- [ ] Performance benchmarks (batch vs sequential)
- [ ] Documentation

---

## 6. Optimization 5: Memory-Efficient Data Loading

### 6.1 Objective

Optimize memory usage when loading and processing large datasets.

### 6.2 Implementation: Lazy Loading & Generators

**File:** `phentrieve/data_processing/lazy_loaders.py` (NEW)

```python
"""
Lazy loading utilities for memory-efficient data processing.

Provides generators and iterators for processing large datasets
without loading everything into memory.
"""

import logging
from typing import Iterator, List, Dict, Any, Optional
from pathlib import Path
import json

logger = logging.getLogger(__name__)


def iter_documents_from_file(
    file_path: Path,
    batch_size: int = 100,
) -> Iterator[List[str]]:
    """
    Iterate over documents in a file (one per line) in batches.

    Memory-efficient: Yields batches without loading entire file.

    Args:
        file_path: Path to file with documents (one per line)
        batch_size: Number of documents per batch

    Yields:
        Batches of document texts
    """
    batch = []

    with open(file_path) as f:
        for line in f:
            line = line.strip()
            if line:
                batch.append(line)

                if len(batch) >= batch_size:
                    yield batch
                    batch = []

        # Yield remaining documents
        if batch:
            yield batch


def iter_jsonl_documents(
    file_path: Path,
    text_field: str = "text",
    batch_size: int = 100,
) -> Iterator[List[Dict[str, Any]]]:
    """
    Iterate over JSONL file in batches.

    Args:
        file_path: Path to JSONL file
        text_field: Field name containing text
        batch_size: Number of documents per batch

    Yields:
        Batches of document dicts
    """
    batch = []

    with open(file_path) as f:
        for line in f:
            try:
                doc = json.loads(line)
                batch.append(doc)

                if len(batch) >= batch_size:
                    yield batch
                    batch = []
            except json.JSONDecodeError as e:
                logger.warning(f"Skipping invalid JSON line: {e}")

    if batch:
        yield batch


def iter_directory_files(
    directory: Path,
    pattern: str = "*.txt",
    batch_size: int = 10,
) -> Iterator[List[Path]]:
    """
    Iterate over files in directory in batches.

    Args:
        directory: Directory path
        pattern: Glob pattern
        batch_size: Number of files per batch

    Yields:
        Batches of file paths
    """
    batch = []

    for file_path in directory.glob(pattern):
        batch.append(file_path)

        if len(batch) >= batch_size:
            yield batch
            batch = []

    if batch:
        yield batch


class LazyDocumentDataset:
    """
    Lazy-loading document dataset.

    Supports:
    - Large files that don't fit in memory
    - Multiple file formats (text, JSONL)
    - Batch iteration
    - Progress tracking
    """

    def __init__(
        self,
        source: Path,
        format: str = "text",  # text, jsonl
        text_field: str = "text",
        batch_size: int = 100,
    ):
        """
        Initialize lazy dataset.

        Args:
            source: File or directory path
            format: Data format
            text_field: Field containing text (for jsonl)
            batch_size: Batch size for iteration
        """
        self.source = Path(source)
        self.format = format
        self.text_field = text_field
        self.batch_size = batch_size

    def __iter__(self) -> Iterator[List[str]]:
        """Iterate over document batches."""
        if self.format == "text":
            yield from iter_documents_from_file(
                self.source,
                batch_size=self.batch_size,
            )
        elif self.format == "jsonl":
            for batch_dicts in iter_jsonl_documents(
                self.source,
                text_field=self.text_field,
                batch_size=self.batch_size,
            ):
                yield [d[self.text_field] for d in batch_dicts]
        else:
            raise ValueError(f"Unknown format: {self.format}")

    def estimate_size(self) -> Optional[int]:
        """
        Estimate number of documents (if possible).

        Returns:
            Estimated document count or None
        """
        if not self.source.is_file():
            return None

        # Count lines (rough estimate)
        num_lines = 0
        with open(self.source) as f:
            for _ in f:
                num_lines += 1

        return num_lines
```

### 6.3 Usage

```python
from phentrieve.data_processing.lazy_loaders import LazyDocumentDataset
from phentrieve.text_processing.batch_processor import BatchDocumentProcessor

# Process large file without loading into memory
dataset = LazyDocumentDataset(
    source=Path("large_corpus.txt"),
    format="text",
    batch_size=100,
)

processor = BatchDocumentProcessor()

for batch in dataset:
    results = processor.process_documents(batch)
    # Process results...
```

### 6.4 Deliverables

- [ ] Implement lazy loading utilities
- [ ] Add `LazyDocumentDataset` class
- [ ] Unit tests for lazy loading
- [ ] Memory usage benchmarks
- [ ] Documentation with usage examples

---

## 7. Optimization 6: ChromaDB Query Optimization

### 7.1 Objective

**ONLY implement if profiling shows ChromaDB is a bottleneck!**

Optimize ChromaDB query patterns for better performance.

### 7.2 Profiling Target

```bash
# Profile ChromaDB query performance
phentrieve profiling baseline corpus.txt --num-docs 100

# Analyze results
grep -i "chroma" results/profiling/baseline/baseline_cpu_report.txt
```

### 7.3 Potential Optimizations (if needed)

**Batch Queries:**
```python
# Instead of: N individual queries
for chunk in chunks:
    results = collection.query(chunk)  # N database calls

# Use: Single batched query
results = collection.query(chunks)  # 1 database call
```

**Connection Pooling:**
- Reuse ChromaDB client instances
- Avoid repeated connection overhead

**Index Optimization:**
- Tune HNSW parameters for better recall/speed trade-off
- Adjust collection metadata

### 7.4 Implementation (TBD based on profiling)

**Only implement if profiling shows ChromaDB is top 3 bottleneck!**

---

## 8. Implementation Roadmap

### Phase 1: Profiling Infrastructure (Week 1)

**Day 1-2: Core Profiling Tools**
- [ ] Implement `PerformanceProfiler` class
- [ ] Add `TimingContext` utility
- [ ] Unit tests for profiling utilities

**Day 3-4: CLI Commands**
- [ ] Implement `phentrieve profiling baseline` command
- [ ] Implement `phentrieve profiling compare` command
- [ ] Implement `phentrieve profiling visualize` command
- [ ] Integration tests

**Day 5: Baseline Profiling**
- [ ] Run baseline profiling on representative workload
- [ ] Analyze results and identify top 3 bottlenecks
- [ ] Document findings in `plan/PROFILING-RESULTS.md`
- [ ] Prioritize optimizations based on measured impact

**Week 1 Deliverables:**
- ✅ Profiling infrastructure
- ✅ Baseline performance measurements
- ✅ Prioritized optimization list (data-driven!)

---

### Phase 2: Core Optimizations (Week 2)

**Implement optimizations IN ORDER of measured impact from profiling!**

**If embedding generation is bottleneck:**
- [ ] Implement parallel embedding generation
- [ ] Add configuration parameters
- [ ] Unit tests
- [ ] Performance benchmarks
- [ ] Update documentation

**If model loading is bottleneck:**
- [ ] Implement `ModelPool` singleton
- [ ] Update `load_embedding_model()` to use pooling
- [ ] Thread-safety tests
- [ ] Memory usage benchmarks

**If batch processing is slow:**
- [ ] Implement `BatchDocumentProcessor`
- [ ] Add `process-batch` CLI command
- [ ] Integration tests
- [ ] Performance benchmarks

**Week 2 Deliverables:**
- ✅ Top 2-3 bottlenecks addressed
- ✅ Measured performance improvements
- ✅ All tests passing

---

### Phase 3: Memory & Data Optimizations (Week 3)

**If memory usage is an issue:**
- [ ] Implement lazy loading utilities
- [ ] Add `LazyDocumentDataset` class
- [ ] Memory benchmarks

**If ChromaDB is bottleneck:**
- [ ] Analyze ChromaDB query patterns
- [ ] Implement optimizations (batching, pooling, etc.)
- [ ] Performance benchmarks

**Week 3 Deliverables:**
- ✅ Memory-efficient data loading
- ✅ ChromaDB optimizations (if needed)
- ✅ Complete performance validation

---

### Timeline Summary

```
Phase 1: Week 1   ████████░░░░░░░░░░  Profiling (CRITICAL!)
Phase 2: Week 2   ░░░░░░░░████████░░  Core optimizations (data-driven)
Phase 3: Week 3   ░░░░░░░░░░░░░░████  Memory & data (if needed)

Total: 2-3 weeks
```

**Key Principle: Profile → Measure → Optimize → Validate → Repeat**

---

## 9. Performance Validation

### 9.1 Validation Methodology

**Before/After Comparison:**

```python
# 1. Establish baseline
phentrieve profiling baseline corpus.txt --num-docs 100 --output baseline_before

# 2. Implement optimization

# 3. Re-profile
phentrieve profiling baseline corpus.txt --num-docs 100 --output baseline_after

# 4. Compare
phentrieve profiling compare baseline_before/baseline.prof baseline_after/baseline.prof
```

### 9.2 Success Criteria

**Performance Targets (from CHUNKING-OPTIMIZATION-PLAN.md):**

| Metric | Baseline | Target | Validation |
|--------|----------|--------|------------|
| **Batch Processing (10+ docs)** | 2-5 sec/doc | 0.5-1.5 sec/doc | ≥2x speedup |
| **Model Loading Overhead** | N × load_time | 1 × load_time | (N-1) × savings |
| **Peak Memory Usage** | Baseline | -30 to -50% | Memory profiler |
| **Long Documents (>1000 words)** | O(n²) | O(n log n) | Algorithmic analysis |

### 9.3 Regression Testing

**Add performance regression tests:**

```python
# tests_new/performance/test_performance_regression.py

import pytest
from phentrieve.text_processing import process_text
from phentrieve.profiling import TimingContext


def test_batch_processing_performance():
    """Ensure batch processing meets performance targets."""
    # Test with 100 documents
    documents = [SAMPLE_TEXT] * 100

    with TimingContext("Batch processing 100 docs") as timer:
        # Process batch
        ...

    # Should complete in <150 seconds (1.5s per doc target)
    assert timer.elapsed_ms < 150_000, (
        f"Batch processing too slow: {timer.elapsed_ms}ms "
        f"(target: <150,000ms)"
    )


def test_model_loading_not_repeated():
    """Ensure model is not loaded multiple times."""
    from phentrieve.embeddings import ModelPool

    pool = ModelPool()
    pool.clear_cache()

    # Load model twice
    model1 = pool.get_model("all-MiniLM-L6-v2")
    model2 = pool.get_model("all-MiniLM-L6-v2")

    # Should be same instance
    assert model1 is model2

    # Should only have 1 model in cache
    assert pool.get_cache_info()["num_cached_models"] == 1
```

### 9.4 Documentation

**Document ALL performance improvements in `plan/PROFILING-RESULTS.md`:**

```markdown
# Profiling Results & Performance Improvements

## Baseline (Before Optimizations)

- **Date:** 2025-01-18
- **Workload:** 100 documents, avg 500 words
- **Total Time:** 450 seconds (4.5s per document)
- **Peak Memory:** 4.2GB

**Top Bottlenecks:**
1. Model loading: 35% of total time
2. Embedding generation: 40% of total time
3. ChromaDB queries: 15% of total time

## Optimization 1: Model Pooling

- **Implementation Date:** 2025-01-20
- **Impact:** 35% time reduction (model loading eliminated)
- **Memory Savings:** 60% (4.2GB → 1.7GB)

## Optimization 2: Parallel Embeddings

- **Implementation Date:** 2025-01-22
- **Impact:** 3.2x speedup for embedding generation
- **Total Speedup:** 2.8x overall (4.5s → 1.6s per document)

## Final Performance (After All Optimizations)

- **Total Time:** 160 seconds (1.6s per document)
- **Peak Memory:** 1.7GB
- **Overall Improvement:** 2.8x faster, 60% less memory
```

---

## Appendix A: Profiling Cheat Sheet

### Quick Profiling Commands

```bash
# Establish baseline
phentrieve profiling baseline corpus.txt

# Profile specific component
python -m cProfile -o output.prof -m phentrieve text process input.txt

# Analyze profile
python -m pstats output.prof
> sort cumtime
> stats 30

# Compare profiles
phentrieve profiling compare before.prof after.prof

# Visualize call graph
phentrieve profiling visualize output.prof --format svg
```

### Reading Profile Output

```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      100    0.123    0.001    5.678    0.057 chunkers.py:850(chunk)
                     ^^^^^^    ^^^^^
                     Per call   Total time (FOCUS ON THIS!)
```

**Focus on `cumtime` (cumulative time) for optimization targets!**

---

## Appendix B: Dependencies

**New Dependencies (minimal):**

```toml
# pyproject.toml

[project.optional-dependencies]
profiling = [
    "gprof2dot>=2022.7.29",  # For call graph visualization
    # graphviz must be installed via system package manager
]
```

**No other new dependencies required!** All optimizations use existing libraries.

---

**END OF TECHNICAL OPTIMIZATIONS PLAN**
