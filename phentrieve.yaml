# Phentrieve Configuration
# This is a sample configuration file with default values
# You can modify this file directly, or copy it to ~/.phentrieve/phentrieve.yaml

# =============================================================================
# DATA PATH CONFIGURATION
# =============================================================================
# These paths will be created automatically if they don't exist

# Main data directory for HPO files, terms, and graph data
data_dir: "data"

# Directory for ChromaDB vector indexes
index_dir: "data/indexes"

# Directory for benchmark results
results_dir: "data/results"

# =============================================================================
# HARDWARE & PERFORMANCE
# =============================================================================

device: "cpu"  # Options: "cuda", "cpu", or specific device like "cuda:0" (Note: "auto" is not supported by PyTorch)
use_fp16: false  # Use half-precision floating point for better performance when available

# =============================================================================
# EMBEDDING MODELS
# =============================================================================

# Default embedding model for retrieval
default_model: "FremyCompany/BioLORD-2023-M"

# Alternative embedding models (for reference/testing)
# jina_model: "jinaai/jina-embeddings-v2-base-de"  # German-specific
# biolord_model: "FremyCompany/BioLORD-2023-M"  # Biomedical domain-specific

# =============================================================================
# RETRIEVAL SETTINGS
# =============================================================================

# Minimum similarity score to display results (0.0-1.0)
min_similarity_threshold: 0.3

# Default number of results to return
default_top_k: 10

# Similarity calculation method
# Options: "hybrid", "cosine", "dot_product"
similarity_formula: "hybrid"

# =============================================================================
# RE-RANKING SETTINGS
# =============================================================================

# Enable cross-encoder re-ranking for improved accuracy
enable_reranker: false

# Re-ranking mode
# Options: "cross-lingual" (query in target lang → English HPO terms)
#          "monolingual" (query in target lang → HPO terms in same lang)
reranker_mode: "cross-lingual"

# Cross-encoder model for re-ranking
# BAAI/bge-reranker-v2-m3: Multilingual cross-encoder (568M params)
# - Trained on MS MARCO relevance datasets
# - Supports 100+ languages for cross-lingual retrieval
# - Used with protected two-stage retrieval (preserves high-confidence dense matches)
reranker_model: "BAAI/bge-reranker-v2-m3"

# Monolingual cross-encoder (monolingual mode - German)
monolingual_reranker_model: "ml6team/cross-encoder-mmarco-german-distilbert-base"

# Number of candidates to retrieve before re-ranking
rerank_candidate_count: 50

# Protected dense retrieval threshold (0.0-1.0)
# High-confidence dense matches (≥threshold) are protected from reranker demotion
# Default: 0.7 (preserves BioLORD's strong cross-lingual semantic matches)
dense_trust_threshold: 0.7

# =============================================================================
# TEXT PROCESSING
# =============================================================================

# Default language for text processing
default_language: "en"

# =============================================================================
# BENCHMARK SETTINGS
# =============================================================================

benchmark:
  # Similarity threshold for benchmark evaluation
  similarity_threshold: 0.1

  # K values for hit rate calculation (Recall@K, Precision@K)
  k_values: [1, 3, 5, 10]

  # Output settings
  save_summaries: true
  save_detailed_results: true
  create_visualizations: true

  # Models to benchmark (evaluated in order)
  models:
    - "FremyCompany/BioLORD-2023-M"  # Domain-specific biomedical model
    - "jinaai/jina-embeddings-v2-base-de"  # Language-specific (German)
    - "T-Systems-onsite/cross-en-de-roberta-sentence-transformer"  # Cross-lingual (EN-DE)
    - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    - "sentence-transformers/distiluse-base-multilingual-cased-v2"
    - "BAAI/bge-m3"
    - "Alibaba-NLP/gte-multilingual-base"
    - "sentence-transformers/LaBSE"
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# =============================================================================
# HPO DATA CONFIGURATION
# =============================================================================

hpo_data:
  # HPO ontology version to download
  # Version string from HPO releases (e.g., "v2025-03-03")
  version: "v2025-03-03"

  # Base URL for HPO releases on GitHub
  base_url: "https://github.com/obophenotype/human-phenotype-ontology/releases/download"

  # Download timeout in seconds
  # Maximum time to wait for HPO data download to complete
  download_timeout: 60

  # Chunk size for streaming downloads (in bytes)
  # Larger chunks = faster downloads but more memory usage
  chunk_size: 8192
